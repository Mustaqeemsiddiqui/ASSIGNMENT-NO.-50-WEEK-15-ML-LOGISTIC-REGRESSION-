{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f3d47c-f632-4d2f-aa87-7f3e7493f1bb",
   "metadata": {},
   "source": [
    "**Q1. Explain the difference between linear regression and logistic regression models. Provide an example of\n",
    "a scenario where logistic regression would be more appropriate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173564c7-ac1b-4968-81e5-9b7c42d7c201",
   "metadata": {},
   "source": [
    "**ANSWER:-----**\n",
    "\n",
    "Linear regression and logistic regression are both statistical models used for different types of predictive tasks. Here’s a breakdown of their differences and an example scenario for logistic regression:\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "**Purpose:** Linear regression is used to predict a continuous dependent variable based on one or more independent variables.\n",
    "\n",
    "**Output:** It produces a continuous output (e.g., predicting house prices, temperatures, etc.).\n",
    "\n",
    "**Equation:** The relationship is modeled using a linear equation:\n",
    "\\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n + \\epsilon \\]\n",
    "where \\( Y \\) is the dependent variable, \\( X_i \\) are the independent variables, \\( \\beta_i \\) are the coefficients, and \\( \\epsilon \\) is the error term.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "**Purpose:** Logistic regression is used to predict a categorical dependent variable, often binary (i.e., two possible outcomes).\n",
    "\n",
    "**Output:** It produces probabilities that can be mapped to binary outcomes (e.g., yes/no, pass/fail, win/lose).\n",
    "\n",
    "**Equation:** The relationship is modeled using the logistic function (sigmoid function):\n",
    "\\[ P(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n)}} \\]\n",
    "where \\( P(Y=1) \\) is the probability that the dependent variable \\( Y \\) equals 1 (the event of interest).\n",
    "\n",
    "### Example Scenario for Logistic Regression\n",
    "\n",
    "**Scenario:** Predicting whether a student will pass or fail a course based on study hours, attendance, and prior grades.\n",
    "\n",
    "**Why Logistic Regression?** The outcome (pass/fail) is categorical. Logistic regression will model the probability of passing (or failing) based on the input variables, allowing us to classify each student into one of the two categories.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "1. **Nature of the Outcome:**\n",
    "   - **Linear Regression:** Predicts a continuous outcome.\n",
    "   - **Logistic Regression:** Predicts a categorical outcome.\n",
    "\n",
    "2. **Error Distribution:**\n",
    "   - **Linear Regression:** Assumes that the residuals (errors) are normally distributed.\n",
    "   - **Logistic Regression:** Does not make such an assumption; instead, it uses the logistic function to bound the probabilities between 0 and 1.\n",
    "\n",
    "3. **Model Interpretation:**\n",
    "   - **Linear Regression:** Coefficients represent the change in the dependent variable for a one-unit change in the independent variable.\n",
    "   - **Logistic Regression:** Coefficients represent the change in the log odds of the dependent variable for a one-unit change in the independent variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d88286-a575-440b-9bc8-65db1760dbb8",
   "metadata": {},
   "source": [
    "**Q2. What is the cost function used in logistic regression, and how is it optimized?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e880127-d7cd-4b71-b377-699c624c7af0",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "The cost function used in logistic regression is the **logistic loss function**, also known as the **log-loss** or **cross-entropy loss**. This function is optimized to find the best-fitting parameters for the logistic regression model.\n",
    "\n",
    "### Logistic Loss Function (Log-Loss)\n",
    "\n",
    "For binary classification, the log-loss function is defined as:\n",
    "\n",
    "\\[ \\text{Log-Loss} = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\]\n",
    "\n",
    "where:\n",
    "- \\( m \\) is the number of training examples,\n",
    "- \\( y_i \\) is the actual label for the \\( i \\)-th training example (0 or 1),\n",
    "- \\( \\hat{y}_i \\) is the predicted probability of the \\( i \\)-th training example being in class 1.\n",
    "\n",
    "The log-loss penalizes incorrect predictions with larger errors, particularly when a predicted probability is far from the actual outcome.\n",
    "\n",
    "### Optimization\n",
    "\n",
    "The optimization of the log-loss function in logistic regression is typically done using **gradient descent** or one of its variants (e.g., stochastic gradient descent, mini-batch gradient descent).\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "1. **Initialize Parameters:** Start with initial values for the parameters (weights) \\(\\beta_0, \\beta_1, \\ldots, \\beta_n\\).\n",
    "\n",
    "2. **Compute Predictions:** For each training example, compute the predicted probability \\(\\hat{y}_i\\) using the logistic function:\n",
    "   \\[ \\hat{y}_i = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\ldots + \\beta_nX_{in})}} \\]\n",
    "\n",
    "3. **Compute the Gradient:** Calculate the gradient of the log-loss with respect to each parameter \\(\\beta_j\\). The gradient for a parameter \\(\\beta_j\\) is:\n",
    "   \\[ \\frac{\\partial \\text{Log-Loss}}{\\partial \\beta_j} = -\\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i) X_{ij} \\]\n",
    "   where \\( X_{ij} \\) is the value of the \\( j \\)-th feature for the \\( i \\)-th training example.\n",
    "\n",
    "4. **Update Parameters:** Update each parameter using the gradient and a learning rate \\(\\alpha\\):\n",
    "   \\[ \\beta_j := \\beta_j - \\alpha \\frac{\\partial \\text{Log-Loss}}{\\partial \\beta_j} \\]\n",
    "\n",
    "5. **Iterate:** Repeat the steps of computing predictions, calculating the gradient, and updating parameters until convergence (i.e., when changes in the cost function are below a certain threshold or after a fixed number of iterations).\n",
    "\n",
    "### Convergence Criteria\n",
    "\n",
    "The optimization process continues until one of the following criteria is met:\n",
    "- The change in the cost function is smaller than a predefined threshold.\n",
    "- A maximum number of iterations is reached.\n",
    "- The gradient values become sufficiently small.\n",
    "\n",
    "By minimizing the log-loss function through gradient descent, logistic regression finds the optimal parameters that best fit the training data, providing accurate probabilities for the binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116eb980-e4b4-4541-a3be-57810eec5c46",
   "metadata": {},
   "source": [
    "**Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2898b91b-d838-4f9e-917a-09255a5f0fb1",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "\n",
    "Regularization is a technique used in logistic regression (and other machine learning models) to prevent overfitting by adding a penalty term to the cost function. This penalty discourages the model from fitting too closely to the training data, which can lead to poor generalization on unseen data.\n",
    "\n",
    "### Types of Regularization\n",
    "\n",
    "The two most common forms of regularization in logistic regression are **L1 regularization** and **L2 regularization**.\n",
    "\n",
    "1. **L1 Regularization (Lasso):**\n",
    "   - Adds the absolute value of the coefficients to the cost function.\n",
    "   - The regularized cost function becomes:\n",
    "     \\[ \\text{Log-Loss} + \\lambda \\sum_{j=1}^{n} |\\beta_j| \\]\n",
    "   - Encourages sparsity, meaning it can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "2. **L2 Regularization (Ridge):**\n",
    "   - Adds the squared value of the coefficients to the cost function.\n",
    "   - The regularized cost function becomes:\n",
    "     \\[ \\text{Log-Loss} + \\frac{\\lambda}{2} \\sum_{j=1}^{n} \\beta_j^2 \\]\n",
    "   - Tends to shrink coefficients evenly, without necessarily driving them to zero.\n",
    "\n",
    "### Combined Regularization (Elastic Net)\n",
    "\n",
    "- Combines L1 and L2 regularization:\n",
    "  \\[ \\text{Log-Loss} + \\lambda_1 \\sum_{j=1}^{n} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{n} \\beta_j^2 \\]\n",
    "- Allows for a balance between sparsity and coefficient shrinkage.\n",
    "\n",
    "### How Regularization Prevents Overfitting\n",
    "\n",
    "1. **Penalizes Complexity:** Regularization adds a penalty for large coefficients, which typically indicates a more complex model. By constraining the size of the coefficients, the model is forced to be simpler, reducing the likelihood of overfitting the noise in the training data.\n",
    "\n",
    "2. **Bias-Variance Trade-off:** Regularization introduces a bias into the model, but this bias can result in a lower variance. While the training error might increase slightly due to the bias, the overall generalization error (performance on unseen data) is reduced.\n",
    "\n",
    "3. **Feature Selection (L1 Regularization):** L1 regularization can drive some coefficients to zero, effectively removing irrelevant or redundant features. This simplifies the model and reduces the risk of overfitting.\n",
    "\n",
    "### Implementation in Logistic Regression\n",
    "\n",
    "Regularization is typically controlled by a parameter \\(\\lambda\\) (or \\(\\alpha\\) in some contexts), which determines the strength of the penalty. A higher \\(\\lambda\\) value results in stronger regularization, while a lower \\(\\lambda\\) value reduces the regularization effect.\n",
    "\n",
    "### Optimized Cost Function with Regularization\n",
    "\n",
    "- **L1 Regularization:**\n",
    "  \\[ \\text{Log-Loss} + \\lambda \\sum_{j=1}^{n} |\\beta_j| \\]\n",
    "\n",
    "- **L2 Regularization:**\n",
    "  \\[ \\text{Log-Loss} + \\frac{\\lambda}{2} \\sum_{j=1}^{n} \\beta_j^2 \\]\n",
    "\n",
    "- **Gradient Descent Updates:** When applying gradient descent, the updates to the coefficients incorporate the regularization term. For L2 regularization, the update rule for a parameter \\(\\beta_j\\) is:\n",
    "  \\[ \\beta_j := \\beta_j - \\alpha \\left( \\frac{\\partial \\text{Log-Loss}}{\\partial \\beta_j} + \\lambda \\beta_j \\right) \\]\n",
    "  For L1 regularization, the update rule includes a sub-gradient due to the non-differentiability at zero.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Regularization helps logistic regression models generalize better by:\n",
    "- Penalizing large coefficients and complex models.\n",
    "- Introducing a bias that reduces variance.\n",
    "- Simplifying the model by potentially eliminating irrelevant features (L1 regularization).\n",
    "\n",
    "By balancing the model complexity and ensuring that it does not fit the training data too closely, regularization effectively mitigates the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcfb131-10cf-4d60-a249-f9c0841ab571",
   "metadata": {},
   "source": [
    "**Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression\n",
    "model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f61a52-2ecb-403d-984d-8b811fc2d63e",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "The **Receiver Operating Characteristic (ROC) curve** is a graphical representation used to evaluate the performance of a binary classification model, such as logistic regression. It illustrates the trade-off between the true positive rate (sensitivity) and the false positive rate (1-specificity) at various threshold settings.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **True Positive Rate (TPR) / Sensitivity / Recall:**\n",
    "   - TPR = \\(\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\\)\n",
    "   - Measures the proportion of actual positives correctly identified by the model.\n",
    "\n",
    "2. **False Positive Rate (FPR):**\n",
    "   - FPR = \\(\\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}}\\)\n",
    "   - Measures the proportion of actual negatives incorrectly identified as positives by the model.\n",
    "\n",
    "3. **Threshold:**\n",
    "   - The ROC curve is plotted by varying the decision threshold. For logistic regression, the threshold determines the cutoff point for classifying a probability prediction as positive or negative. \n",
    "\n",
    "### Plotting the ROC Curve\n",
    "\n",
    "To plot the ROC curve:\n",
    "\n",
    "1. **Calculate Predictions:** Use the logistic regression model to predict probabilities for the positive class on the test set.\n",
    "   \n",
    "2. **Vary Threshold:** Adjust the decision threshold from 0 to 1. For each threshold value, calculate the TPR and FPR.\n",
    "\n",
    "3. **Plot Points:** Plot TPR against FPR for each threshold value, resulting in the ROC curve.\n",
    "\n",
    "### Interpreting the ROC Curve\n",
    "\n",
    "- **Perfect Classifier:** A perfect classifier would have a point at (0, 1), meaning it has 100% sensitivity (no false negatives) and 0% FPR (no false positives).\n",
    "- **Random Classifier:** A classifier that makes random guesses would produce a diagonal line from (0, 0) to (1, 1), indicating no better performance than random chance.\n",
    "- **Good Classifier:** The closer the ROC curve is to the top left corner, the better the model's performance. This indicates high sensitivity and low FPR.\n",
    "\n",
    "### Area Under the ROC Curve (AUC-ROC)\n",
    "\n",
    "- **AUC-ROC:** The Area Under the ROC Curve (AUC-ROC) is a single scalar value summarizing the performance of the model. It ranges from 0 to 1.\n",
    "  - **AUC = 1:** Perfect model.\n",
    "  - **AUC = 0.5:** No discriminative power, equivalent to random guessing.\n",
    "  - **AUC > 0.5:** Better than random guessing; the higher the AUC, the better the model's performance.\n",
    "\n",
    "### Using the ROC Curve to Evaluate Logistic Regression\n",
    "\n",
    "1. **Visual Assessment:** The shape of the ROC curve provides a visual assessment of the model's ability to distinguish between the positive and negative classes.\n",
    "2. **Threshold Selection:** The ROC curve helps in choosing an optimal threshold that balances sensitivity and specificity according to the specific needs of the problem.\n",
    "3. **Model Comparison:** AUC-ROC is useful for comparing different models. A model with a higher AUC is generally better at classification.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a logistic regression model predicting whether patients have a certain disease (positive class) based on various features.\n",
    "\n",
    "1. **Predict Probabilities:** The model outputs probabilities for each patient.\n",
    "2. **Vary Threshold:** Calculate TPR and FPR for thresholds ranging from 0 to 1.\n",
    "3. **Plot ROC Curve:** Create a plot of TPR vs. FPR at different thresholds.\n",
    "4. **Calculate AUC-ROC:** Compute the area under the ROC curve to quantify the overall performance.\n",
    "\n",
    "The ROC curve and AUC-ROC provide a comprehensive way to evaluate and compare the effectiveness of logistic regression models, ensuring that they are not only accurate but also balanced in terms of sensitivity and specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1515ee5-a330-43ac-9ec7-0b34e3ae6fc0",
   "metadata": {},
   "source": [
    "**Q5. What are some common techniques for feature selection in logistic regression? How do these\n",
    "techniques help improve the model's performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa90524-0550-4d04-b9e5-20db796356ae",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "\n",
    "Feature selection is a crucial step in building a logistic regression model, as it helps in improving model performance by eliminating irrelevant or redundant features. Here are some common techniques for feature selection in logistic regression:\n",
    "\n",
    "### 1. **Filter Methods**\n",
    "\n",
    "Filter methods assess the relevance of features by looking at their statistical properties, independently of the model.\n",
    "\n",
    "- **Correlation Matrix:**\n",
    "  - Calculate the correlation coefficients between each feature and the target variable.\n",
    "  - Select features with high correlation (absolute value) with the target.\n",
    "  - Also, check for multicollinearity (high correlation between features) and remove highly correlated features.\n",
    "\n",
    "- **Chi-Square Test:**\n",
    "  - For categorical features, use the chi-square test to determine the independence between the feature and the target variable.\n",
    "  - Select features with low p-values (indicating a significant relationship with the target).\n",
    "\n",
    "- **ANOVA (Analysis of Variance):**\n",
    "  - For numerical features, use ANOVA to compare the means of different groups and select features with significant differences.\n",
    "\n",
    "### 2. **Wrapper Methods**\n",
    "\n",
    "Wrapper methods evaluate the performance of a subset of features by training and testing a model.\n",
    "\n",
    "- **Forward Selection:**\n",
    "  - Start with no features and iteratively add the most significant feature at each step.\n",
    "  - Evaluate model performance (e.g., using cross-validation) and stop when adding more features does not significantly improve performance.\n",
    "\n",
    "- **Backward Elimination:**\n",
    "  - Start with all features and iteratively remove the least significant feature at each step.\n",
    "  - Continue until removing features no longer improves model performance.\n",
    "\n",
    "- **Recursive Feature Elimination (RFE):**\n",
    "  - Train the model and rank features based on their importance.\n",
    "  - Remove the least important features recursively and evaluate model performance at each step.\n",
    "  - Stop when performance no longer improves significantly.\n",
    "\n",
    "### 3. **Embedded Methods**\n",
    "\n",
    "Embedded methods perform feature selection during the model training process.\n",
    "\n",
    "- **L1 Regularization (Lasso):**\n",
    "  - Adds a penalty equal to the absolute value of the coefficients.\n",
    "  - Encourages sparsity by driving some coefficients to zero, effectively selecting a subset of features.\n",
    "\n",
    "- **L2 Regularization (Ridge):**\n",
    "  - Adds a penalty equal to the squared value of the coefficients.\n",
    "  - While it does not perform feature selection, it reduces the impact of less important features.\n",
    "\n",
    "- **Elastic Net:**\n",
    "  - Combines L1 and L2 regularization.\n",
    "  - Balances feature selection (L1) and coefficient shrinkage (L2).\n",
    "\n",
    "### 4. **Dimensionality Reduction Techniques**\n",
    "\n",
    "These techniques transform the feature space into a lower-dimensional space while retaining most of the information.\n",
    "\n",
    "- **Principal Component Analysis (PCA):**\n",
    "  - Transforms the original features into a smaller set of uncorrelated components.\n",
    "  - Select components that explain the most variance in the data.\n",
    "\n",
    "- **Linear Discriminant Analysis (LDA):**\n",
    "  - Aims to maximize the separation between multiple classes.\n",
    "  - Transforms features into a lower-dimensional space based on class separability.\n",
    "\n",
    "### How Feature Selection Improves Model Performance\n",
    "\n",
    "1. **Reduces Overfitting:**\n",
    "   - By removing irrelevant or redundant features, the model is less likely to fit noise in the training data, improving generalization to unseen data.\n",
    "\n",
    "2. **Improves Model Interpretability:**\n",
    "   - A simpler model with fewer features is easier to interpret and understand, providing clearer insights.\n",
    "\n",
    "3. **Enhances Model Training Efficiency:**\n",
    "   - Fewer features reduce the computational cost and time required for training the model.\n",
    "\n",
    "4. **Mitigates Multicollinearity:**\n",
    "   - Feature selection helps in reducing multicollinearity, ensuring that the model coefficients are more stable and reliable.\n",
    "\n",
    "5. **Boosts Model Performance:**\n",
    "   - By retaining only the most relevant features, the model can achieve better predictive performance.\n",
    "\n",
    "By employing these feature selection techniques, logistic regression models can become more robust, interpretable, and efficient, leading to better overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce61eb-6fab-4cae-98ad-313c56f5433c",
   "metadata": {},
   "source": [
    "**Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
    "with class imbalance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0315937-fa1b-4d5d-8a17-69b66a40d0e5",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "\n",
    "Handling imbalanced datasets is crucial in logistic regression as it ensures the model performs well across all classes, especially the minority class. Here are some strategies to deal with class imbalance:\n",
    "\n",
    "### 1. **Resampling Techniques**\n",
    "\n",
    "#### a. **Oversampling the Minority Class**\n",
    "- **Random Oversampling:** Randomly duplicate instances of the minority class to balance the class distribution.\n",
    "- **SMOTE (Synthetic Minority Over-sampling Technique):** Generate synthetic samples for the minority class by interpolating between existing samples.\n",
    "\n",
    "#### b. **Undersampling the Majority Class**\n",
    "- **Random Undersampling:** Randomly remove instances of the majority class to balance the class distribution.\n",
    "- **Tomek Links and NearMiss:** Techniques that intelligently undersample the majority class by focusing on borderline examples or nearest neighbors.\n",
    "\n",
    "### 2. **Algorithmic Approaches**\n",
    "\n",
    "#### a. **Cost-Sensitive Learning**\n",
    "- Modify the learning algorithm to incorporate different costs for misclassification errors, giving higher penalty to misclassifying the minority class.\n",
    "- **Weighted Loss Function:** Adjust the loss function to assign higher weights to the minority class:\n",
    "  \\[ \\text{Weighted Log-Loss} = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ w_i y_i \\log(\\hat{y}_i) + w_i (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\]\n",
    "  where \\( w_i \\) is the weight for the \\( i \\)-th instance.\n",
    "\n",
    "### 3. **Evaluation Metrics**\n",
    "\n",
    "#### a. **Use Appropriate Metrics**\n",
    "- **Precision, Recall, and F1-Score:** Evaluate the model using metrics that are sensitive to class imbalance.\n",
    "- **ROC-AUC:** Use the area under the ROC curve to evaluate the model’s ability to distinguish between classes.\n",
    "- **Precision-Recall Curve:** Especially useful for imbalanced datasets, as it focuses on the performance with respect to the minority class.\n",
    "\n",
    "### 4. **Generating Synthetic Data**\n",
    "\n",
    "#### a. **SMOTE Variants**\n",
    "- **Borderline-SMOTE:** Focus on generating synthetic samples near the decision boundary.\n",
    "- **ADASYN (Adaptive Synthetic Sampling):** Generate more synthetic samples for harder-to-classify instances.\n",
    "\n",
    "### 5. **Ensemble Methods**\n",
    "\n",
    "#### a. **Balanced Random Forests**\n",
    "- Combine resampling techniques with random forests by resampling the dataset in each iteration of tree building.\n",
    "- Ensure each tree in the ensemble is built on a balanced subset of the data.\n",
    "\n",
    "#### b. **EasyEnsemble and BalanceCascade**\n",
    "- **EasyEnsemble:** Create multiple balanced subsets by undersampling the majority class and train separate classifiers on each subset, then aggregate their predictions.\n",
    "- **BalanceCascade:** Iteratively train classifiers, removing correctly classified majority class instances at each step, thus focusing on harder examples.\n",
    "\n",
    "### 6. **Adjusting Decision Threshold**\n",
    "\n",
    "- **Threshold Moving:** Adjust the classification threshold to favor the minority class. Instead of using the default threshold of 0.5, find an optimal threshold that improves recall or F1-score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f69f1d6-2a54-47bb-ae48-663de1d137a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.23.5)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cd0e8a3-6d0b-4673-af9f-a61b27718349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after SMOTE and PCA:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.51      0.49       136\n",
      "           1       0.57      0.53      0.55       164\n",
      "\n",
      "    accuracy                           0.52       300\n",
      "   macro avg       0.52      0.52      0.52       300\n",
      "weighted avg       0.53      0.52      0.52       300\n",
      "\n",
      "ROC-AUC: 0.522596843615495\n",
      "\n",
      "Results with cost-sensitive learning:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.58      0.52       136\n",
      "           1       0.57      0.46      0.51       164\n",
      "\n",
      "    accuracy                           0.52       300\n",
      "   macro avg       0.52      0.52      0.52       300\n",
      "weighted avg       0.53      0.52      0.52       300\n",
      "\n",
      "ROC-AUC: 0.5221484935437589\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame({\n",
    "    'feature1': np.random.randn(1000),\n",
    "    'feature2': np.random.randn(1000),\n",
    "    'feature3': np.random.randn(1000),\n",
    "})\n",
    "y = np.random.randint(0, 2, 1000)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Strategy 1: Resampling with SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_res_scaled = scaler.fit_transform(X_res)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA to reduce multicollinearity (optional)\n",
    "pca = PCA(n_components=2)  # Adjust the number of components as needed\n",
    "X_res_pca = pca.fit_transform(X_res_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Train logistic regression on resampled data\n",
    "model = LogisticRegression(penalty='l2', C=1.0)\n",
    "model.fit(X_res_pca, y_res)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test_pca)\n",
    "print(\"Results after SMOTE and PCA:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Strategy 2: Cost-sensitive learning\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "model = LogisticRegression(class_weight=class_weights_dict)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nResults with cost-sensitive learning:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a28130-3ec8-443f-a3a4-8c8a6336c04c",
   "metadata": {},
   "source": [
    "**Q7. Can you discuss some common issues and challenges that may arise when implementing logistic\n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
    "among the independent variables?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa4b92-788f-4bb0-b822-690d5025b8d7",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "Implementing logistic regression can come with several issues and challenges. Here are some common ones and their potential solutions:\n",
    "\n",
    "### 1. Multicollinearity\n",
    "\n",
    "#### Issue:\n",
    "Multicollinearity occurs when two or more independent variables are highly correlated, which can lead to unstable coefficient estimates and inflated standard errors.\n",
    "\n",
    "#### Solutions:\n",
    "- **Remove Highly Correlated Variables:** Identify and remove one of the correlated variables using a correlation matrix.\n",
    "- **Principal Component Analysis (PCA):** Transform the correlated variables into a smaller set of uncorrelated components.\n",
    "- **Ridge Regression:** Use L2 regularization, which can mitigate the effect of multicollinearity by shrinking the coefficients.\n",
    "\n",
    "### 2. Overfitting\n",
    "\n",
    "#### Issue:\n",
    "Overfitting occurs when the model performs well on the training data but poorly on unseen data due to capturing noise and outliers.\n",
    "\n",
    "#### Solutions:\n",
    "- **Regularization:** Apply L1 (Lasso) or L2 (Ridge) regularization to penalize large coefficients.\n",
    "- **Cross-Validation:** Use techniques like k-fold cross-validation to ensure the model generalizes well to unseen data.\n",
    "- **Simplify the Model:** Remove irrelevant features or use feature selection methods to reduce the complexity of the model.\n",
    "\n",
    "### 3. Imbalanced Datasets\n",
    "\n",
    "#### Issue:\n",
    "Class imbalance can lead to a model that performs well on the majority class but poorly on the minority class.\n",
    "\n",
    "#### Solutions:\n",
    "- **Resampling Techniques:** Use oversampling (e.g., SMOTE) or undersampling to balance the class distribution.\n",
    "- **Class Weights:** Assign higher weights to the minority class in the cost function.\n",
    "- **Evaluation Metrics:** Use metrics like precision, recall, F1-score, and ROC-AUC instead of accuracy.\n",
    "\n",
    "### 4. Non-Linearity\n",
    "\n",
    "#### Issue:\n",
    "Logistic regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable, which may not always hold true.\n",
    "\n",
    "#### Solutions:\n",
    "- **Feature Engineering:** Create interaction terms or polynomial features to capture non-linear relationships.\n",
    "- **Non-Linear Models:** Consider using non-linear models like decision trees or neural networks if the relationship is highly non-linear.\n",
    "\n",
    "### 5. Outliers\n",
    "\n",
    "#### Issue:\n",
    "Outliers can disproportionately affect the model, leading to skewed results.\n",
    "\n",
    "#### Solutions:\n",
    "- **Identify and Remove Outliers:** Use statistical tests or visualization methods to detect and remove outliers.\n",
    "- **Robust Scalers:** Use robust scalers that are less sensitive to outliers (e.g., median-based scaling).\n",
    "\n",
    "### 6. Missing Data\n",
    "\n",
    "#### Issue:\n",
    "Missing data can reduce the amount of available data and introduce bias.\n",
    "\n",
    "#### Solutions:\n",
    "- **Imputation:** Use techniques like mean/mode/median imputation, k-nearest neighbors, or model-based imputation to fill in missing values.\n",
    "- **Remove Missing Data:** If the amount of missing data is small, consider removing those instances.\n",
    "\n",
    "### 7. Model Interpretability\n",
    "\n",
    "#### Issue:\n",
    "Understanding and interpreting the coefficients in logistic regression can be challenging, especially with many features.\n",
    "\n",
    "#### Solutions:\n",
    "- **Standardization:** Standardize the features to make the coefficients comparable.\n",
    "- **Odds Ratios:** Transform the coefficients into odds ratios to make them more interpretable.\n",
    "- **Partial Dependence Plots:** Use these plots to understand the relationship between each feature and the target variable.\n",
    "\n",
    "### 8. Convergence Issues\n",
    "\n",
    "#### Issue:\n",
    "The logistic regression model may not converge if the learning algorithm fails to find optimal parameters, especially with large datasets or complex models.\n",
    "\n",
    "#### Solutions:\n",
    "- **Feature Scaling:** Standardize or normalize the features to ensure they are on a similar scale.\n",
    "- **Algorithm Parameters:** Adjust the solver and maximum iterations parameters in the learning algorithm.\n",
    "- **Simplify the Model:** Reduce the number of features or use regularization to stabilize the optimization process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c124251a-42d4-4dea-8482-8efb105d2b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.57      0.51       136\n",
      "           1       0.56      0.45      0.50       164\n",
      "\n",
      "    accuracy                           0.50       300\n",
      "   macro avg       0.51      0.51      0.50       300\n",
      "weighted avg       0.51      0.50      0.50       300\n",
      "\n",
      "ROC-AUC: 0.5086979913916786\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame({\n",
    "    'feature1': np.random.randn(1000),\n",
    "    'feature2': np.random.randn(1000),\n",
    "    'feature3': np.random.randn(1000),\n",
    "})\n",
    "y = np.random.randint(0, 2, 1000)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA to reduce multicollinearity\n",
    "pca = PCA(n_components=2)  # Adjust the number of components as needed\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Train logistic regression with L2 regularization\n",
    "model = LogisticRegression(penalty='l2', C=1.0)\n",
    "model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test_pca)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368346d2-1d03-4c5d-9123-868e849952d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
